{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Initiation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install bs4\n",
        "# %pip install html2text\n",
        "# %pip install clean-text\n",
        "# %pip install undetected-chromedriver requests\n",
        "# %pip install selenium[\"4.18.1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5762/410238637.py:29: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
            "  from IPython.core.display import display, HTML\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import bs4\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import urllib.parse\n",
        "from selenium import webdriver as wd2\n",
        "import undetected_chromedriver as webdriver\n",
        "from cleantext import clean\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from itertools import product\n",
        "import html2text\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor, MarkupLMModel\n",
        "import os\n",
        "import gc\n",
        "pd.set_option('display.max_columns', None)\n",
        "# from umap import UMAP\n",
        "import matplotlib.ticker as plticker\n",
        "import matplotlib.pyplot as plt\n",
        "import sweetviz as sv\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN, OPTICS, KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import requests\n",
        "from IPython.core.display import display, HTML\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# import accelerate\n",
        "# from g4f.client import Client\n",
        "import bs4\n",
        "# import nltk\n",
        "# from nltk.stem import PorterStemmer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# import nltk\n",
        "import re\n",
        "import json\n",
        "# from googlesearch import search\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_of_sources = 100\n",
        "std_header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "could not detect version_main.therefore, we are assuming it is chrome 108 or higher\n"
          ]
        }
      ],
      "source": [
        "driver_path = './chromedriver'\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--use_subprocess\")\n",
        "chrome_options.add_argument(\"javascript.enabled\")\n",
        "browser = webdriver.Chrome(driver_executable_path='./chromedriver', options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "wait = WebDriverWait(browser, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "sup_words = ['reveiws', 'descriptions', 'marks']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_trust_pylot_by_category(category: str, supword: str='review', border: int=50, body='res'):\n",
        "    t = urllib.parse.quote(category + ' ' + supword, safe='')\n",
        "    v= f\"https://www.google.com/search?channel=fs&client=ubuntu-sn&q={t}\"\n",
        "    browser.get(f\"https://www.google.com/search?channel=fs&client=ubuntu-sn&q={t}\")\n",
        "    wait.until(EC.presence_of_element_located((By.ID, body)))\n",
        "    r = browser.find_element(by=By.ID, value=body)\n",
        "    x = []\n",
        "    for i in r.find_elements(by=By.TAG_NAME, value='a'):\n",
        "        x.append(i.get_attribute('href'))\n",
        "    return x[:border]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s-L9jcaskf99"
      },
      "outputs": [],
      "source": [
        "def selenium_google_search_source(text: str, browser, border: int=20, body='rcnt'): # 'rcnt'\n",
        "    t = urllib.parse.quote(text, safe='')\n",
        "    v= f\"https://www.google.com/search?channel=fs&client=ubuntu-sn&q={t}\"\n",
        "    browser.get(f\"https://www.google.com/search?channel=fs&client=ubuntu-sn&q={t}\")\n",
        "    x = []\n",
        "    for i in browser.find_element(by=By.ID, value=body).find_elements(by=By.TAG_NAME, value='a'):\n",
        "        x.append(i.get_attribute('href'))\n",
        "    return x[:border]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def selenium_source_link_to_html(links: List[str], browser):\n",
        "    html_source = []\n",
        "    bad_res = []\n",
        "    for k, i in enumerate(links):\n",
        "        try:\n",
        "            browser.get(i)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            html_source.append(browser.page_source)\n",
        "        except Exception as e:\n",
        "            if type(e) is KeyboardInterrupt:\n",
        "                break\n",
        "            print('*', end='')\n",
        "            bad_res.append(i)\n",
        "        print(k, end=', ')\n",
        "    return html_source, bad_res\n",
        "\n",
        "\n",
        "def req_source_link_to_html(links: List[str], browser):\n",
        "    html_source = []\n",
        "    bad_res = []\n",
        "    for k, i in enumerate(links):\n",
        "        try:\n",
        "            r = requests.get(i, headers=std_header)\n",
        "            if r.status_code == 200:\n",
        "                html_source.append(r.text)\n",
        "        except Exception as e:\n",
        "            if type(e) is KeyboardInterrupt:\n",
        "                break\n",
        "            # print('*', end='')\n",
        "            bad_res.append(i)\n",
        "        # print(k, end=', ')\n",
        "    return html_source, bad_res\n",
        "\n",
        "\n",
        "\n",
        "def req_source_link_to_html_saving(links: List[str], req_id, browser):\n",
        "    html_source = []\n",
        "    bad_res = []\n",
        "    for k, i in enumerate(links):\n",
        "        try:\n",
        "            r = requests.get(i, headers=std_header, timeout=40)\n",
        "            if r.status_code == 200:\n",
        "                with open(f'./html_pages/{req_id}_{k}.html', 'w') as f:\n",
        "                    f.write(r.text)\n",
        "        except Exception as e:\n",
        "            if type(e) is KeyboardInterrupt:\n",
        "                break\n",
        "            # print('*', end='')\n",
        "            bad_res.append(i)\n",
        "        # print(k, end=', ')\n",
        "    return html_source, bad_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_requests(search_text: list[str], border=50):\n",
        "    bad_res = []\n",
        "    result_search = dict()\n",
        "    for i, j in enumerate(search_text):\n",
        "        try:\n",
        "            sleep(1)\n",
        "            x = selenium_google_search_source(j, browser=browser, border=border)\n",
        "            result_search[j] = dict(enumerate(filter(lambda a: a != None, x)))\n",
        "        except Exception as e:\n",
        "            if type(e) is KeyboardInterrupt:\n",
        "                break\n",
        "            print('*', end='')\n",
        "            bad_res.append(j)\n",
        "        print(i, end=', ')\n",
        "\n",
        "    return result_search, bad_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "r = requests.get('https://www.trustpilot.com/categories', headers=std_header).text\n",
        "categories = bs4.BeautifulSoup(r, 'html.parser').find(\"section\", class_='categories_categoriesWrapper__fIt_K').find_all('ul')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_categories = sum([[i.text for i in i.find_all('li')] for i in categories], [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./trust_pilot_cat.json', 'w') as f:\n",
        "    json.dump({'subcategories': sub_categories}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./trust_pilot_cat.json', 'r') as f:\n",
        "    categories = json.load(f)['subcategories']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "567"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(categories) * 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "try: \n",
        "    with open('links_lists.json', 'r') as f:\n",
        "        res = json.load(f)\n",
        "except:\n",
        "    res = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "567it [00:37, 15.19it/s] \n"
          ]
        }
      ],
      "source": [
        "for c, sw in tqdm(product(categories, sup_words)):\n",
        "    # sleep(1)\n",
        "    k = c + ' '  + sw\n",
        "    if (not k in res) or (list(res.keys())[-1] == k):\n",
        "        res[k] = search_trust_pylot_by_category(c, sw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('links_lists.json', 'w') as f:\n",
        "    json.dump(res, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('links_lists_formated.json', \"+a\", encoding = \"utf-8\") as file:\n",
        "    json.dump(res, file, ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parse Trust Pilot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('links_lists.json', 'r') as f:\n",
        "        res = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "trustpilot_link_index_pair = dict()\n",
        "for vs in res.values():\n",
        "    for i, v in enumerate(vs):\n",
        "        o = urlparse(v)\n",
        "        if 'trustpilot' in str(o.netloc):\n",
        "            trustpilot_link_index_pair[v] = i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('trustpilot_link_index_pair.json', \"+a\", encoding = \"utf-8\") as file:\n",
        "    json.dump(trustpilot_link_index_pair, file, ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Trust_pilot_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./trustpilot_link_index_pair.json', 'r') as f:\n",
        "    trustpilot_link_index_pair = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 218/218 [03:23<00:00,  1.07it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(trustpilot_link_index_pair):\n",
        "    o = urlparse(i)\n",
        "    fn = f'{str(o.netloc.replace(\".\", \"_\"))}____{str(o.path).replace(\"/\", \"_\")}____{str(o.query).replace(\"=\", \"_\")}____{trustpilot_link_index_pair[i]}.html'\n",
        "    r = requests.get(i, headers=std_header).text\n",
        "    with open(f'tp_htmls/{fn}', 'w') as f:\n",
        "        f.write(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
        "model = MarkupLMModel.from_pretrained(\"microsoft/markuplm-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def removetextbetweentags(html):\n",
        "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
        "    \n",
        "    # Remove all text between nodes\n",
        "    for element in soup.find_all(text=True):\n",
        "        if isinstance(element, bs4.Comment):\n",
        "            continue  # Skip comments\n",
        "        element.replace_with('')\n",
        "    \n",
        "    # Remove contents of all script and style tags\n",
        "    for script_or_style in soup(['script', 'style']):\n",
        "        script_or_style.clear()\n",
        "    \n",
        "    return str(soup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/218 [00:00<?, ?it/s]/tmp/ipykernel_5762/3579530471.py:5: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  for element in soup.find_all(text=True):\n",
            "100%|██████████| 218/218 [02:13<00:00,  1.63it/s]\n"
          ]
        }
      ],
      "source": [
        "window = 1000\n",
        "for fn in tqdm(os.listdir('./tp_htmls/')):\n",
        "    with open(f'./tp_htmls/{fn}', 'r') as f:\n",
        "        # r = bs4.BeautifulSoup(f.read(), 'html.parser')\n",
        "        r = bs4.BeautifulSoup(removetextbetweentags(f.read()), 'html.parser')\n",
        "        r = r.prettify()\n",
        "        l = r.split('\\n')\n",
        "        # r_p = str(r)\n",
        "        r_p = [''.join(l[i:i+window]) for i in range(0, len(l), window)]\n",
        "    k = list(map(lambda a: model(**processor(a, return_tensors=\"pt\")).last_hidden_state.squeeze(), r_p))\n",
        "    outputs = torch.cat(k, 0)\n",
        "    torch.save(outputs, f'./vectorized_torch_seq/{fn.replace(\"json\", \"\")}.pt')\n",
        "    del outputs\n",
        "    # del l\n",
        "    # del r_p\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
